{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Techniques And Its Types Assignment-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Random Forest Regressor?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# A Random Forest Regressor is a powerful machine learning algorithm used for regression tasks.\n",
    "# It's an ensemble method, meaning it combines the predictions of multiple individual models to make a final \n",
    "# prediction. Specifically, it's an ensemble of decision trees.   \n",
    "\n",
    "# The base learners in a Random Forest are decision trees. A decision tree is a tree-like structure \n",
    "# where each internal node represents a feature, each branch represents a decision rule, and each leaf node \n",
    "# represents the outcome (in this case, a predicted value).\n",
    "\n",
    "# How it Works (Step-by-Step):\n",
    "\n",
    "# Bootstrap Sampling: Create multiple (e.g., hundreds or thousands) bootstrap samples from the original training data.\n",
    "\n",
    "# Tree Construction: For each bootstrap sample:\n",
    "# Build a decision tree.\n",
    "# At each node of the tree, randomly select a subset of features.   \n",
    "# Choose the best feature from this subset to split the data.\n",
    "# Grow the tree until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf).   \n",
    "\n",
    "# Prediction: To make a prediction for a new data point:\n",
    "# Pass the data point down each of the decision trees in the forest.   \n",
    "# Each tree will produce a prediction (a numerical value).   \n",
    "# Average the predictions from all the trees to get the final prediction.   \n",
    "\n",
    "# Key Advantages of Random Forest Regressor:\n",
    "# High Accuracy: Random Forests often provide very good predictive accuracy, often outperforming single decision trees.   \n",
    "# Reduces Overfitting: The combination of bootstrap sampling and feature randomness helps to reduce overfitting, \n",
    "# making the model more robust and generalizable to unseen data.   \n",
    "# Handles High Dimensionality: Random Forests can handle datasets with a large number of features effectively.   \n",
    "# No Feature Scaling Required: Decision trees, and therefore Random Forests, do not require feature scaling.   \n",
    "# Provides Feature Importance: Random Forests can provide estimates of feature importance, indicating which\n",
    "# features are most influential in making predictions.   \n",
    "\n",
    "# Key Disadvantages:\n",
    "# Computational Cost: Training a Random Forest can be computationally expensive, especially with a large number \n",
    "# of trees or a large dataset.   \n",
    "# Less Interpretable: Random Forests are less interpretable than single decision trees. It's harder to \n",
    "# understand exactly why a particular prediction was made.   \n",
    "# Memory Usage: Storing a large number of trees can require significant memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# The Random Forest Regressor employs a two-pronged approach to mitigate the risk of overfitting, \n",
    "# a common issue in machine learning where models learn the training data too well, including its noise, \n",
    "# and fail to generalize to unseen data.   \n",
    "\n",
    "# 1. Bootstrap Sampling:\n",
    "# Creating Diverse Training Sets: Each decision tree within the Random Forest is trained on a unique bootstrap \n",
    "# sample of the original training data. This involves randomly selecting data points with replacement, meaning\n",
    "# some data points may appear multiple times in a tree's training set, while others may be omitted.   \n",
    "# Reducing Sensitivity to Specific Data Points: This process ensures that each tree is exposed to a slightly \n",
    "# different version of the training data. As a result, each tree learns different aspects of the data and is \n",
    "# less likely to overfit to specific, potentially noisy data points in the original training set.   \n",
    "\n",
    "# 2. Feature Randomness:\n",
    "# Decorrelating Trees: At each node of a decision tree, the best split is chosen from a random subset of features,\n",
    "# rather than considering all possible features.   \n",
    "# Preventing Over-reliance on Single Features: This helps to decorrelate the trees in the forest. \n",
    "# If all trees were allowed to choose the best split from all features, they might end up choosing similar \n",
    "# features, leading to similar tree structures and potentially overfitting to those dominant features. \n",
    "# By considering only a subset of features at each split, the trees are forced to consider different features,\n",
    "# making them more diverse and less prone to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# The final prediction of the Random Forest is obtained by averaging the predictions of all the individual \n",
    "# decision trees. This averaging process smooths out the predictions and reduces the impact of any \n",
    "# individual tree that might have overfit to its specific training set.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are the hyperparameters of Random Forest Regressor?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# 1. Related to Individual Trees:\n",
    "\n",
    "# n_estimators: The number of trees in the forest. A larger number of trees generally improves performance\n",
    "# (up to a point), but also increases computational cost. This is often the first hyperparameter to tune.   \n",
    "# max_depth: The maximum depth of a tree. Controlling the depth helps prevent overfitting. A smaller \n",
    "# depth can lead to underfitting.   \n",
    "# min_samples_split: The minimum number of samples required to split an internal node. Higher values can \n",
    "# help prevent overfitting.   \n",
    "# min_samples_leaf: The minimum number of samples required to be at a leaf node. Similar to min_samples_split,\n",
    "# this helps prevent overfitting.   \n",
    "# max_features: The number of features to consider when looking for the best split at each node. This introduces \n",
    "# randomness and helps decorrelate the trees. Common options include:\n",
    "# \"sqrt\": Square root of the total number of features.\n",
    "# \"log2\": Base-2 logarithm of the total number of features.\n",
    "# None: Consider all features (less random, can lead to overfitting).\n",
    "# Integer or float: Specify the exact number or proportion of features.   \n",
    "# criterion: The function to measure the quality of a split. For regression, common options are:\n",
    "# \"squared_error\" (or \"mse\"): Mean squared error.\n",
    "# \"absolute_error\" (or \"mae\"): Mean absolute error.\n",
    "# \"poisson\": Poisson regression loss.\n",
    "# max_leaf_nodes: Maximum number of leaf nodes a tree can have. Another way to control tree size \n",
    "# and prevent overfitting.\n",
    "\n",
    "# 2. Related to Ensemble Creation:\n",
    "\n",
    "# bootstrap: Whether or not to use bootstrap samples when building trees. Generally, this should be True \n",
    "# (the default) for Random Forests. Setting it to False creates a Random Trees regressor.\n",
    "# oob_score: Whether to use out-of-bag samples to estimate the generalization error. This can be useful \n",
    "# for tuning n_estimators.\n",
    "\n",
    "# 3. Other Important Parameters:\n",
    "\n",
    "# random_state: Controls the randomness of the sampling and tree building. Setting a seed ensures reproducibility.\n",
    "# n_jobs: The number of jobs to run in parallel. -1 means use all available processors. This can significantly \n",
    "# speed up training.\n",
    "# warm_start: Whether to reuse the solutions from previous calls to fit and add more estimators to the ensemble.\n",
    "# Useful for incremental learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Random Forest Regressor:\n",
    "# Ensemble of Trees: A Random Forest Regressor builds an ensemble of multiple decision trees. Each tree is trained \n",
    "# on a different bootstrap sample of the data and considers a random subset of features at each split.   \n",
    "# Averaging Predictions: The final prediction of a Random Forest is obtained by averaging the predictions of \n",
    "# all the individual trees in the forest.   \n",
    "# Reduces Overfitting: The combination of bootstrap sampling and feature randomness significantly reduces the \n",
    "# risk of overfitting. The diverse set of trees helps to smooth out the predictions and make the model more robust.   \n",
    "# Reduces Variance: Random Forests have lower variance compared to individual decision trees. The averaging \n",
    "# process reduces the sensitivity to small changes in the training data.   \n",
    "# Balances Bias and Variance: Random Forests offer a better balance between bias and variance. \n",
    "# They can capture complex relationships while also generalizing well to unseen data.\n",
    "\n",
    "\n",
    "# Decision Tree Regressor:\n",
    "# Single Tree: A Decision Tree Regressor builds a single tree-like model to predict continuous values. \n",
    "# It recursively partitions the data based on feature values to create branches and ultimately leaf nodes \n",
    "# that represent predicted values.   \n",
    "# Greedy Approach: It uses a greedy approach to choose the best split at each node, aiming to minimize the \n",
    "# variance or mean squared error within the resulting sub-partitions.   \n",
    "# Prone to Overfitting: Decision trees are highly susceptible to overfitting, especially when they are deep \n",
    "# and complex. They can memorize the training data, including noise, and fail to generalize well to unseen data.   \n",
    "# High Variance: Decision trees have high variance, meaning they are sensitive to small changes in the \n",
    "# training data. A slight change in the data can lead to a significantly different tree structure.   \n",
    "# Low Bias (if complex enough): Decision trees can have low bias if they are allowed to grow deep enough. \n",
    "# They can capture complex relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Advantages of Random Forest Regressor:\n",
    "# High Accuracy: Random Forests often provide very good predictive accuracy, often outperforming single decision trees.   \n",
    "# Reduces Overfitting: The combination of bootstrap sampling and feature randomness helps to reduce overfitting, \n",
    "# making the model more robust and generalizable to unseen data.   \n",
    "# Handles High Dimensionality: Random Forests can handle datasets with a large number of features effectively.   \n",
    "# No Feature Scaling Required: Decision trees, and therefore Random Forests, do not require feature scaling.   \n",
    "# Provides Feature Importance: Random Forests can provide estimates of feature importance, indicating which\n",
    "# features are most influential in making predictions.   \n",
    "\n",
    "# Disadvantages:\n",
    "# Computational Cost: Training a Random Forest can be computationally expensive, especially with a large number \n",
    "# of trees or a large dataset.   \n",
    "# Less Interpretable: Random Forests are less interpretable than single decision trees. It's harder to \n",
    "# understand exactly why a particular prediction was made.   \n",
    "# Memory Usage: Storing a large number of trees can require significant memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What is the output of Random Forest Regressor?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# The output of a Random Forest Regressor is a continuous value, which is evaluated by averaging all\n",
    "# the individual decision tree regressors predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Can Random Forest Regressor be used for classification tasks?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# No, the Random Forest Regressor is specifically designed for regression tasks, where the target \n",
    "# variable is continuous (a numerical value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
